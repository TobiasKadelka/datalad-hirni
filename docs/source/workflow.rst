.. _chap_workflow:

Basic workflow
**************

Datalad-hirni comes with a set of :ref:`commands <chap_commands>` aiming to support the following workflow to curate a
study dataset and to convert it. This workflow to a degree reflects the envisioned structure of a study dataset as
described in the :ref:`concepts <chap_concepts>` section.

This is a somewhat abstract description of what it is supposed to do. It might be more convenient for you to first see
what *you* have to do by looking at the examples, showing this exact workflow: The creation of a study dataset
:ref:`here <chap_demos_study>` and afterwards the :ref:`conversion <chap_demos_conversion>`.

Build your study dataset
========================

In order to build such a dataset that binds all your raw data, the first thing to do is to create a new dataset. To set
it up as a hirni dataset, you can use a builtin routine called ``cfg_hirni`` which is implemented as a Datalad procedure_.
Ideally you create your dataset right at the moment you start (planning) your study. Even without any actual data, there
is basic metadata you might already be able to capture by (partially) filling ``dataset_description.json``, ``README``,
``CHANGELOG`` etc. Hirni's webUI might be of help here.

The idea is then to add all data to the dataset as it comes into existence. That is, for each acquisition import the
DICOMs, import all additional data, possibly edit the specification. It's like writing documentation for your code: If
you don't do it at the beginning, chances are you'll never properly do it at all.
You can always edit and add things later, of course.

.. _procedure: https://datalad.readthedocs.io/en/latest/generated/man/datalad-run-procedure.html


Import DICOM files
==================

Importing the DICOMS consists of several steps. The ``hirni-import-dcm`` command will help you, given you can provide it a
tarball containing all DICOMs of an acquisition (internal structure of the tarball doesn't matter). Of course you can
achieve the same result differently.
The first step is to retrive the tarball, of course, extract its content and create a dataset from it. If you passed an
acquisition directory to the command it will create this dataset in ``dicoms/`` underneath that directory. Otherwise it
is created at a temporary location.
Then the DICOM metadata is extracted. If the acquisition directory wasn't given, a name for the aquisition is derived
from that metadata (how exactly this is done is configurable) the respective directory created and the dataset is moved
into it from its temporary location.
Either way there's a new subdataset beneath the respective acquisition directory by now and it provides extracted DICOM
metadata. Note, that the metadata doesn't technically describe DICOM files, but rather image series that are found in
those files. The final step is now to use that metadata to derive a specification. This is done by ``hirni-dicom2spec``,
which automatically is called by ``hirni-import-dcm``. However, if you need to skip ``hirni-Ã¬mport-dcm`` for whatever
reason (say you already have a DICOM dataset you want to use instead of creating a new one by such a tarball), you can
run ``hirni-dicom2spec``. How the rule system is used to derive the specification deserves its own chaper (at least if
you wish to adjust those rules). This should now result in a ``studyspec.json`` within the respective acquisition
directory. You can now review the autogenerated entries and correct or enhance them.

Add arbitrary data
==================






Convert your dataset
====================







